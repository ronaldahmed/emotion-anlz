%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Advanced Issues on NLP - Emotion Classification}

\author{Ronald Cardenas Acosta\\
  Faculty of Information and Communication Technology \\
  University of Malta \\
  {\tt ronald.cardenas.18@um.edu.mt} \\}

\date{\today}

\begin{document}
\maketitle
%\begin{abstract}
%  This document contains th
%\end{abstract}

\section{Introduction}

The analysis of discrete emotion categories has received increasing attention in last few years \cite{alm2005emotions,aman2007identifying,dodds2011temporal}.
Even though comprehensive pychological models of emotion categorization have been proposed \cite{ekman1999basic, plutchik2001nature}, the task remains challenging from a machine learning perspective as noted in recent shared tasks \cite{mohammad2016semeval}. More recently, the field has openend to wider application scenarios such as muti-lingual setups \cite{barbieri2018semeval} and irony detection \cite{van2018semeval}.

In this report we tackle the task of discrete emotion recognition over social media text.
We follow the categorization of emotions proposed by \citet{ekman1999basic} and \citet{plutchik2001nature}, considering the following seven categories:
anger, disgust, fear, joy, sadness, surprise, and trust.

We take a nominal, multi-label classification approach to the problem, i.e.\ a single instance is labeled with a set of emotion classes.
We compare the performance of three kinds of classifiers over the SSEC dataset \cite{schuff2017annotation}.
All code have been made available for reproduction at \url{https://github.com/ronaldahmed/emotion-anlz}.
%
%\section{Problem Formulation}
%
%We tackle the task of emotion analysis as a multi-label classification problem.
%Given a 
%, i.e.\ given a 


\section{Experimental Setup}

\subsection{Dataset}

We use the version of the Stance Sentiment Emotion Corpus, SSEC \cite{schuff2017annotation},  extracted from the provided aggregated file ({\it unified-dataset.csv}).
The dataset consists of 4,868 tweets annotated in a multi-label setup with 7 kinds of emotions: anger, disgust, fear, joy,  sadness, surprise, and trust.
We held out 800 samples for test and keep the rest as training set.
Table~\ref{table:corpus-stats}  presents the distribution of emotion classes over the training and test set. The average number of labels annotated per sample is 3.23 in the training set and 3.24 in the test set.


\begin{table}[]
\centering
\begin{tabular}{|l|r|r|}
\hline
Emotion  & \multicolumn{1}{c|}{Train} & \multicolumn{1}{c|}{Test} \\ \hline
Anger    & 2,432                      & 470                       \\ \hline
Disgust  & 1,834                      & 349                       \\ \hline
Fear     & 1,526                      & 314                       \\ \hline
Joy      & 1,732                      & 335                       \\ \hline
Sadness  & 2,206                      & 438                       \\ \hline
Surprise & 934                        & 174                       \\ \hline
Trust    & 2,245                      & 455                       \\ \hline
\end{tabular}
\caption{Emotion class distribution in the training and test set. Note that samples are annotated in a multi-label setup.}
\label{table:corpus-stats}
\end{table}

\subsection{Feature Extraction and Preprocessing}

For preprocessing, we lowercase and tokenize the text using NLTK library. \footnote{\url{http://www.nltk.org/}}
The vocabulary considered consists of all word types with frequency greater than one in the training set. All other word types in the training and test set are replaced with a special {\it unknown} token.
Every document is represented as a bag of words with stopwords~\footnote{Standard stopword list obtained from NLTK} and out-of-vocabulary words removed.
Features were extracted calculating the document-term matrix with TF-IDF weights. Then, we apply Latent Semantic Analysis (LSA) in order to reduce the dimensionality of features to 500.

\subsection{Models}

We experiment with three models: Random Forest, k-Nearest-Neighbors, and a Multilayer Perceptron (MLP).
We use available implementations from the Scikit-Learn library \footnote{\url{https://scikit-learn.org}} for all experiments.

\subsection{Tuning of hyper-parameters}
We perform random search  of hyper-parameters for 100 iterations. In each iteration, we evaluate a model's performance through 5-fold cross-validation over the training set. The MLP was trained using an Adam optimizer \cite{kingma2014adam} with L2-regularization.
Table~\ref{table-tuning} presents the hyperparameters and their respective explored ranges, for all models.


\begin{table*}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Model                 & Hyper-parameter                     & Range                                                                                                                                          & Optimal value \\ \hline
Random Forest         & Number of estimators                & {[}10--100{]}                                                                                                                                   & 80            \\ \cline{2-4} 
                      & Criterion                           & {[}gini, entropy{]}                                                                                                                            & entropy       \\ \cline{2-4} 
                      & Maximum depth                       & {[}10--100{]}                                                                                                                                   & 38            \\ \hline
KNN                   & Number of neighbors                 & {[}5--20{]}                                                                                                                                     & 6             \\ \cline{2-4} 
                      & Weights                             & {[}uniform, distance{]}                                                                                                                        & distance      \\ \hline
Muti-layer Perceptron & Batch size                          & {[}20--200{]}                                                                                                                                   & 158           \\ \cline{2-4} 
                      & Learning rate                       & [1e-5 -- 1e-1]                                                                                                                                    & 7.50e-05      \\ \cline{2-4} 
                      & L2-regularization parameter (alpha) & [1e-5 -- 1.0]                                                                                                                                     & 5.99e-05      \\ \cline{2-4} 
                      & Activation function                 & {[}tanh, relu{]}                                                                                                                               & relu          \\ \cline{2-4} 
                      & Hidden layer sizes                  & \begin{tabular}[c]{@{}l@{}}{[}(150),(100),(50),(10),\\ (100,100),(50,50),(10,10),\\ (100,50),(50,100),\\ (50,50,50),(10,10,10){]}\end{tabular} & (100,50)      \\ \hline
\end{tabular}
\caption{Hyper-parameters tuned for each model, alongside the explored ranges and optimal values found for each one.}
\label{table-tuning}
\end{table*}


\section{Results and Discussion}

Table~\ref{table:results} presents classification results for all models investigated. We observe that the MLP obtains significantly better performance than the other two models for 5 out of 7 emotion classes, according to micro and macro-averaged F1 score.
When looking at the F1 scores per class, for classes in which MLP does better, we observe that the difference between MLP and the runner up model ranges between as litte as 0.41 points (MLP vs RF, {\it anger} class) up to 23.42 points (MLP vs KNN, {\it fear} class).
For the {\it sadness} class, RF outperforms MLP by 1.65 points; wheres for the {\it surprise} class, KNN outperforms MLP by 3.39 points.

It is worth noting the low scores obtained for the class {\it surprise}, for which the best perfoming model barely obtaines 11.86 F1-score.
This could be explained by the lesser amount of labeled samples for this class (just 174 in the test set, see Table~\ref{table:corpus-stats}).
In turn, this situation could explain the low recall scores obtained thourghout Table~\ref{table:results}.


\begin{table*}[]
\centering
\begin{tabular}{|l|rrr|rrr|rrr|}
\hline
\multicolumn{1}{|c|}{Emotion class} & \multicolumn{3}{c|}{Random Forest}                                      & \multicolumn{3}{c|}{KNN}                                                & \multicolumn{3}{c|}{MLP}                                                \\
\multicolumn{1}{|c|}{}              & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c|}{F1} & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c|}{F1} \\ \hline
Anger                               & 64.84                 & 92.98                 &  76.40                    & 64.03                 & 75.74                 & 69.4                    & 72.99                 & 81.06                 & {\bf 76.81}                   \\
Disgust                             & 56.63                 & 31.81                 & 40.73                   & 51.04                 & 35.24                 & 41.69                   & 62.25                 & 53.87                 & {\bf 57.76}                   \\
Fear                                & 58.06                 & 5.73                  & 10.43                   & 51.02                 & 15.92                 & 24.27                   & 60.19                 & 39.49                 & {\bf 47.69}                   \\
Joy                                 & 69.93                 & 31.94                 & 43.85                   & 48.21                 & 52.24                 & 50.14                   & 67.51                 & 55.82                 & {\bf 61.11}                   \\
Sadness                             & 63.62                 & 76.26                 & {\bf 69.37}                   & 57.14                 & 41.10                  & 47.81                   & 66.96                 & 68.49                 & 67.72                   \\
Surprise                            & 100.00                   & 0.57                  & 1.14                    & 22.58                 & 8.05                  & {\bf 11.86}                   & 53.33                 & 4.60                   & 8.47                    \\
Trust                               & 59.76                 & 76.7                  & 67.18                   & 61.72                 & 41.10                  & 49.34                   & 70.81                 & 71.43                 & {\bf 71.12}                   \\ \hline
Micro avg                           & 62.71                 & 53.53                 & 57.76                   & 55.99                 & 42.80                  & 48.51                   & 67.88                 & 59.68                 & {\bf 63.52}                   \\
Macro avg                           & 67.55                 & 45.14                 & 44.16                   & 50.82                 & 38.48                 & 42.07                   & 64.86                 & 53.54                 & {\bf 55.81}                   \\ \hline
\end{tabular}
\caption{Classification results per emotion label for all models investigated. Results are presented in terms of precision (P), recall (R), and F1 score.  Best scores are presented in bold.}
\label{table:results}
\end{table*}


% \section{Conclusions}



\bibliography{acl2019}
\bibliographystyle{acl_natbib}


\end{document}
